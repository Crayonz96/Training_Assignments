{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "path = r'C:\\Users\\User\\Documents\\Training Assignments\\ConvNet'\n",
    "train_name = 'mnist_train.csv'\n",
    "test_name = 'mnist_test.csv'\n",
    "\n",
    "\n",
    "training_data = pd.read_csv(os.path.join(path, train_name), sep = ',',header=None)\n",
    "testing_data = pd.read_csv(os.path.join(path,test_name),sep = ',', header=None)\n",
    "\n",
    "####Split the training set into training and validation sets\n",
    "train_data = training_data.sample(frac=0.9, random_state = 42)\n",
    "val_data = training_data.drop(train_data.index, axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "### separate out the output from the input arrays for training, validation and testing sets\n",
    "train_y = train_data.iloc[:,0].values.reshape(-1,1)\n",
    "train_x = train_data.drop(0,axis=1).values.reshape(len(train_data),28,28)\n",
    "\n",
    "val_y = val_data.iloc[:,0].values.reshape(-1,1)\n",
    "val_x = val_data.drop(0,axis=1).values.reshape(len(val_data),28,28)\n",
    "\n",
    "test_y = testing_data.iloc[:,0].values.reshape(-1,1)\n",
    "test_x = testing_data.drop(0,axis=1).values.reshape(len(testing_data),28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define Batch Size\n",
    "X_sample = train_x[:8]\n",
    "Y_sample = train_y[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert output Y into One Hot Vectors\n",
    "def output_one_hot(Y):    \n",
    "    output_list=[]\n",
    "    for value in Y:\n",
    "        output = np.zeros((10,1))\n",
    "        output[value] = value\n",
    "        output_list.append(output)\n",
    "        continue\n",
    "    return np.asarray(output_list)\n",
    "\n",
    "    \n",
    "Y_sample = output_one_hot(Y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define activation functions- Have used only ReLU and Softmax in the code\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def softmax_loss(soft_max,Y):\n",
    "    #cross entropy loss\n",
    "    return -1*np.sum(Y*np.log(soft_max))\n",
    "\n",
    "\n",
    "def softmax(X): \n",
    "    X -= np.max(X) #ensure that gradients don't explode\n",
    "    return  np.exp(X)/np.sum(np.exp(X),axis=0)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### convolution operation for forward pass\n",
    "def cross_correlation(X, kernel, pad_width, stride, kernel_bias):\n",
    "    \n",
    "    row_id = kernel.shape[0] ## vertical length of filter\n",
    "    col_id = kernel.shape[1] ## horizontal length of filter\n",
    "    activation_map = []\n",
    "    \n",
    "    ### get the shape of the output activation_map\n",
    "    n_out_rows = (X.shape[0] + 2*pad_width - row_id)/stride + 1 \n",
    "    n_out_cols = (X.shape[1] + 2*pad_width - col_id)/stride + 1\n",
    "    \n",
    "    ###pad if option is provided\n",
    "    X = np.pad(X,(pad_width, pad_width),'constant', constant_values = (0))\n",
    "    \n",
    "   #### take slices of the input image and then multilpy with filter. Add bias = kernel_bias as well. \n",
    "    # the y index should be in the range 0 to the vertical length with step length = stride \n",
    "    for stride_y in range(0, X.shape[0], stride): \n",
    "        if stride_y + row_id <= X.shape[0]:\n",
    "            ##the x index should be in the range 0 to the horizontal length with step length = stride\n",
    "            for stride_x in range(0, X.shape[1], stride):\n",
    "                if stride_x + col_id <= X.shape[1]:\n",
    "                    activation_map.append(np.sum(X[stride_y:stride_y+row_id, stride_x:stride_x+col_id]*kernel)+\\\n",
    "                                          float(kernel_bias))\n",
    "                    continue        \n",
    "            continue\n",
    "    \n",
    "    ##reshape activation map into appropriate shape\n",
    "    activation_map = np.asarray(activation_map).reshape(n_out_rows, n_out_cols)\n",
    "    \n",
    "    return activation_map    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "### max pooling operation\n",
    "def max_pooling(X, kernel, stride,pad_width = 0):\n",
    "    row_id = kernel.shape[0] ## vertical length of the filter\n",
    "    col_id = kernel.shape[1] ### horizontal length of the filter \n",
    "    pooling_map = []\n",
    "    max_indexes = []\n",
    "\n",
    "    ### compute output shape after pooling    \n",
    "    n_out_rows = (X.shape[0] + 2*pad_width - row_id)/stride + 1\n",
    "    n_out_cols = (X.shape[1] + 2*pad_width - col_id)/stride + 1\n",
    "    \n",
    "    \n",
    "    for stride_y in range(0, X.shape[0], stride):\n",
    "        for stride_x in range(0, X.shape[1], stride):\n",
    "            if stride_y + row_id <= X.shape[0]:\n",
    "                if stride_x + col_id <= X.shape[1]:\n",
    "                    pooling_map.append(np.amax(X[stride_y:stride_y+row_id, stride_x:stride_x+col_id]))\n",
    "                    ###take a small slice and flatten it. Then find the argument of the max value from that so that\\\n",
    "                    ### if multiple entries are present in a slice then only one index is returned.\n",
    "                    temp = X[stride_y:stride_y+row_id, stride_x:stride_x+col_id]\n",
    "                    temp.ravel()\n",
    "                    max_id = np.argmax(temp, axis=0)\n",
    "                    max_indexes.append(max_id)\n",
    "                                    \n",
    "    ### reshape pooled activation_map into computed shape\n",
    "    pooling_map = np.asarray(pooling_map).reshape(n_out_rows, n_out_cols)    \n",
    "\n",
    "    return pooling_map, max_indexes    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "###FORWARD PASS\n",
    "def forward_pass_fc(X,Y, theta_2, bias_2, theta_1, bias_1):\n",
    "    np.random.seed(42)\n",
    "  \n",
    "    #### FULLY CONNECTED LAYER-1 RelU Activation (input from pooling, output to FC2)\n",
    "    fc1_input = X.ravel().reshape(-1,1)\n",
    "    fc1_output = ReLU(np.matmul(theta_1.T, fc1_input) + bias_1)\n",
    "\n",
    "    ####FULLY CONNECTED LAYER-2 SOFTMAX layer (input from FC-1, output to Loss function)\n",
    "    fc2_output = softmax(np.matmul(theta_2.T, fc1_output) + bias_2)\n",
    "    #loss = softmax_loss(fc2_output,Y_sample)\n",
    "    \n",
    "    return fc2_output, fc1_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "####BACKWARD PASS\n",
    "def backpropagation_fc(X, Y, X_sample, theta_2, bias_2, theta_1, bias_1, max_indexes, activation_map,kernel, stride):\n",
    "    \n",
    "    ### Call forward pass\n",
    "    a2, a1 = forward_pass_fc(X, Y, theta_2, bias_2, theta_1, bias_1)\n",
    "    \n",
    "\n",
    "    ####LOSS function backprop\n",
    "    d_a2 = np.reciprocal(a2)\n",
    "    d_z2 = np.zeros(a2.shape).ravel()\n",
    "    \n",
    "\n",
    "    ####SOFTMAX backprop- FULLY CONNECTED LAYER-2\n",
    "    for i in range(len(a2)):\n",
    "        for j in range(len(a2)):\n",
    "            if i==j:\n",
    "                d_z2[i] = a2[i]*(1-a2[i])\n",
    "            else:\n",
    "                d_z2[i] = -1*a2[i]*a2[j]\n",
    "    \n",
    "    d_z2 = d_z2.reshape(-1,1)\n",
    "    d_theta_2 = d_z2*a1.T\n",
    "    d_bias_2 = d_z2\n",
    "    \n",
    "   \n",
    "    ####FULLY CONNECTED LAYER-1 ACTIVATION RelU Backprop \n",
    "    d_z1 = a1.copy()\n",
    "    d_z1[d_z1 <= 0] = 0\n",
    "    d_z1[d_z1 > 0 ] = 1\n",
    "\n",
    "    d_theta_1 = d_z1*X.ravel().reshape(-1,1).T\n",
    "    d_bias_1 = d_z1\n",
    "\n",
    "        \n",
    "    ###GRADIENT OF Flattened output from pooling layer. Input to FC-1\n",
    "    d_flattened_X = np.dot(d_theta_1.T, d_z1)\n",
    "    \n",
    "\n",
    "    #### reshaped into shape of pooling output\n",
    "    d_flattened_X = d_flattened_X.reshape(X.shape)\n",
    "    \n",
    "\n",
    "    ### Flattened X broadcast into shape of output from convolution\n",
    "    d_reshaped_X = d_flattened_X.repeat(2,axis=0).repeat(2,axis=1)\n",
    "    \n",
    "    \n",
    "    ##### Backprop for MAX POOLING Backprop\n",
    "    \n",
    "    ##create an array of zeros of shape activation map = output from convolution \n",
    "    grad_pooling = np.zeros_like(activation_map)\n",
    "    \n",
    "    ## insert ones at locations of max value\n",
    "    for i in max_indexes:\n",
    "        grad_pooling[i] = 1.0\n",
    "\n",
    "      \n",
    "    ### Gradient of MAX POOLING \n",
    "    d_pooling = d_reshaped_X*grad_pooling\n",
    "    \n",
    "        \n",
    "    #####CONVOLUTION ACTIVATION ReLU Backprop\n",
    "    grad_activation_map = d_pooling.copy()\n",
    "    \n",
    "    grad_activation_map[grad_activation_map <= 0] = 0\n",
    "    grad_activation_map[grad_activation_map > 0 ] = 1\n",
    "    \n",
    "    \n",
    "    #####Convolution backprop\n",
    "    row_id = kernel.shape[0] ##vertical length of filter used in convolution\n",
    "    col_id = kernel.shape[1] ##horizontal length of filter used in convolution\n",
    "    \n",
    "\n",
    "    ### initialize gradient of kernel with shape of kernel\n",
    "    d_kernel = np.zeros_like(kernel)\n",
    "    \n",
    "    #### take slices of shape = kernel from the input image and from gradient of the convolution operation. \\\n",
    "    ###Multiply element-wise to get gradient of loss with respect to kernel\n",
    "    for stride_y in range(0, grad_activation_map.shape[0], stride):\n",
    "        for stride_x in range(0, grad_activation_map.shape[1], stride):\n",
    "            if stride_y + row_id <= grad_activation_map.shape[0]:\n",
    "                if stride_x + col_id <= grad_activation_map.shape[1]:\n",
    "                    temp = X_sample[stride_y:stride_y+row_id, stride_x:stride_x+col_id]*\\\n",
    "                    grad_activation_map[stride_y:stride_y+row_id, stride_x:stride_x+col_id]\n",
    "                    d_kernel = d_kernel + temp\n",
    "                    del temp\n",
    "\n",
    "    ### compute Gradient of bias used in convolution operation                    \n",
    "    d_kernel_bias = grad_activation_map.ravel().sum()\n",
    "    \n",
    "    return d_z2, d_theta_2, d_bias_2, d_z1, d_theta_1, d_bias_1, d_kernel, d_kernel_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Function to run the entire neural network\n",
    "def run(X, Y, alpha, num_of_iterations):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    ### initialize the kernel used in convolution\n",
    "    kernel = np.random.randn(3,3)\n",
    "    \n",
    "    \n",
    "    ### intialize the kernel used in max pooling operation\n",
    "    kernel_pool = np.zeros((2,2))\n",
    "    \n",
    "    \n",
    "    #### initialize the bias used in convolution operation\n",
    "    kernel_bias = float(np.random.randn(1))\n",
    "    \n",
    "    ### initialize the weights of FC-1\n",
    "    theta_1 = np.random.randn(196, 5)\n",
    "    bias_1 = np.random.randn(5,1)\n",
    "\n",
    "    ### initialize the weights of FC-2\n",
    "    theta_2 = np.random.randn(5, 10)\n",
    "    bias_2 = np.random.randn(10,1)\n",
    "\n",
    "\n",
    "\n",
    "    training_loss=[]\n",
    "    ### no of epochs\n",
    "    for iter_num in range(num_of_iterations):\n",
    "    \n",
    "        ###for each sample in batch\n",
    "        for X_sample, Y_sample in itertools.izip(X, Y):\n",
    "\n",
    "            ### call convolution operation forward pass\n",
    "            activation_map = cross_correlation(X_sample, kernel, 1, 1, kernel_bias)\n",
    "    \n",
    "            ### call max pooling operation forward pass\n",
    "            fc1_input, max_indexes = max_pooling(activation_map, kernel_pool, 2)\n",
    "        \n",
    "            ### call the forward pass for FC layers\n",
    "            a2, a1  = forward_pass_fc(fc1_input,Y_sample, theta_2, bias_2, theta_1, bias_1)\n",
    "\n",
    "            ### call the backpropagation operations\n",
    "            d_z2, d_theta_2, d_bias_2, d_z1, d_theta_1, d_bias_1, d_kernel, d_kernel_bias = \\\n",
    "            backpropagation_fc(fc1_input,Y_sample,X_sample, theta_2, bias_2, theta_1, bias_1,max_indexes, activation_map,kernel,1)\n",
    "\n",
    "            ### calculate the loss \n",
    "            loss = softmax_loss(softmax(a2), Y_sample)\n",
    "            training_loss.append(loss)\n",
    "        \n",
    "            ### compute alpha/num of training samples and call it modified alpha\n",
    "            mod_alpha = float(alpha)/float(batch_size)\n",
    "                        \n",
    "            ### simulataneous parameter update\n",
    "            kernel = kernel - mod_alpha*d_kernel\n",
    "            kernel_bias = kernel_bias - mod_alpha*d_kernel_bias\n",
    "            theta_1 = theta_1 - mod_alpha*d_theta_1.T\n",
    "            bias_1 = bias_1 - mod_alpha*d_bias_1\n",
    "            theta_2 = theta_2 - mod_alpha*d_theta_2.T\n",
    "            bias_2 = bias_2 - mod_alpha*d_bias_2\n",
    "        \n",
    "            print 'loss', loss\n",
    "            \n",
    "    return training_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: divide by zero encountered in reciprocal\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 17.228051202141327\n",
      "loss 7.383450515203425\n",
      "loss 18.678722549629754\n",
      "loss 21.365603230303886\n",
      "loss 7.383450515203425\n",
      "loss 21.36560692947449\n",
      "loss 17.228051202141327\n",
      "loss 16.21435135813903\n",
      "loss 16.214351669222438\n",
      "loss 6.503260362875205\n",
      "loss 18.678725082013752\n",
      "loss 21.36562542397652\n",
      "loss 6.503257540325697\n",
      "loss 21.36563282114693\n",
      "loss 16.214353535735157\n",
      "loss 16.214353846822657\n",
      "loss 16.214354157910744\n",
      "loss 6.5032528363379996\n",
      "loss 18.678728458450358\n",
      "loss 21.36565501049711\n",
      "loss 6.503250014102257\n",
      "loss 21.365662406226914\n",
      "loss 16.21435602445151\n",
      "loss 16.214356335543684\n",
      "loss 16.214356646636435\n",
      "loss 6.503245310637437\n",
      "loss 18.67873183480077\n",
      "loss 21.365684591255885\n",
      "loss 6.503242488715383\n",
      "loss 21.36569198554549\n",
      "loss 16.214358513205216\n",
      "loss 16.21435882430205\n",
      "loss 16.21435913539947\n",
      "loss 6.503237785773324\n",
      "loss 18.67873521106496\n",
      "loss 21.365714166254488\n",
      "loss 6.503234964164891\n",
      "loss 21.36572155910431\n",
      "loss 16.214361001996206\n",
      "loss 16.2143613130977\n",
      "loss 16.214361624199775\n",
      "loss 6.503230261745474\n",
      "loss 18.678738587242897\n",
      "loss 21.365743735494586\n",
      "loss 6.503227440450592\n",
      "loss 21.36575112690503\n",
      "loss 16.214363490824432\n",
      "loss 16.214363801930578\n",
      "loss 16.2143641130373\n",
      "loss 6.503222738553701\n",
      "loss 18.678741963334552\n",
      "loss 21.365773298977818\n",
      "loss 6.503219917572299\n",
      "loss 21.365780688949307\n",
      "loss 16.21436597968983\n",
      "loss 16.214366290800616\n",
      "loss 16.21436660191198\n",
      "loss 6.503215216197816\n",
      "loss 18.67874533933989\n",
      "loss 21.365802856705844\n",
      "loss 6.503212395529822\n",
      "loss 21.365810245238794\n",
      "loss 16.214368468592344\n",
      "loss 16.214368779707765\n",
      "loss 16.214369090823766\n",
      "loss 6.503207694677631\n",
      "loss 18.678748715258887\n",
      "loss 21.36583240868032\n",
      "loss 6.503204874322977\n",
      "loss 21.365839795775138\n",
      "loss 16.21437095753192\n",
      "loss 16.214371268651963\n",
      "loss 16.214371579772592\n",
      "loss 6.503200173992958\n",
      "loss 18.678752091091507\n",
      "loss 21.365861954902886\n",
      "loss 6.503197353951574\n",
      "loss 21.36586934055999\n",
      "loss 16.214373446508485\n",
      "loss 16.214373757633155\n",
      "loss 16.214374068758403\n",
      "loss 6.503192654143611\n",
      "loss 18.67875546683772\n",
      "loss 21.365891495375205\n",
      "loss 6.503189834415424\n",
      "loss 21.365898879595\n",
      "loss 16.214375935521996\n",
      "loss 16.21437624665128\n",
      "loss 16.21437655778114\n",
      "loss 6.503185135129401\n",
      "loss 18.678758842497498\n",
      "loss 21.365921030098917\n",
      "loss 6.503182315714343\n",
      "loss 21.365928412881825\n",
      "loss 16.214378424572388\n",
      "loss 16.214378735706276\n",
      "loss 16.214379046840744\n",
      "loss 6.503177616950142\n",
      "loss 18.678762218070812\n",
      "loss 21.365950559075678\n",
      "loss 6.503174797848143\n",
      "loss 21.365957940422106\n",
      "loss 16.214380913659607\n",
      "loss 16.214381224798096\n",
      "loss 16.21438153593716\n",
      "loss 6.503170099605647\n",
      "loss 18.678765593557628\n",
      "loss 21.36598008230714\n",
      "loss 6.503167280816635\n",
      "loss 21.365987462217493\n",
      "loss 16.214383402783593\n",
      "loss 16.214383713926672\n",
      "loss 16.214384025070327\n",
      "loss 6.503162583095727\n",
      "loss 18.678768968957915\n",
      "loss 21.36600959979494\n",
      "loss 6.5031597646196335\n",
      "loss 21.366016978269634\n",
      "loss 16.214385891944286\n",
      "loss 16.214386203091948\n",
      "loss 16.214386514240186\n",
      "loss 6.503155067420195\n",
      "loss 18.678772344271646\n",
      "loss 21.36603911154073\n",
      "loss 6.503152249256949\n",
      "loss 21.366046488580185\n",
      "loss 16.214388381141628\n",
      "loss 16.21438869229387\n",
      "loss 16.214389003446684\n",
      "loss 6.5031475525788665\n",
      "loss 18.678775719498788\n",
      "loss 21.366068617546166\n",
      "loss 6.503144734728398\n",
      "loss 21.366075993150787\n",
      "loss 16.214390870375563\n",
      "loss 16.214391181532374\n",
      "loss 16.214391492689757\n",
      "loss 6.503140038571551\n",
      "loss 18.678779094639314\n",
      "loss 21.36609811781288\n",
      "loss 6.503137221033789\n",
      "loss 21.366105491983078\n",
      "loss 16.21439335964603\n",
      "loss 16.214393670807407\n",
      "loss 16.21439398196935\n",
      "loss 6.503132525398064\n",
      "loss 18.678782469693186\n",
      "loss 21.366127612342527\n",
      "loss 6.5031297081729385\n",
      "loss 21.366134985078713\n",
      "loss 16.21439584895298\n",
      "loss 16.21439616011891\n",
      "loss 16.214396471285404\n",
      "loss 6.5031250130582166\n",
      "loss 18.678785844660386\n",
      "loss 21.36615710113675\n",
      "loss 6.503122196145659\n",
      "loss 21.366164472439337\n",
      "loss 16.21439833829634\n",
      "loss 16.21439864946682\n",
      "loss 16.21439896063787\n",
      "loss 6.503117501551822\n",
      "loss 18.678789219540874\n",
      "loss 21.366186584197195\n",
      "loss 6.503114684951761\n",
      "loss 21.366193954066592\n",
      "loss 16.21440082767607\n",
      "loss 16.21440113885109\n",
      "loss 16.214401450026674\n",
      "loss 6.503109990878697\n",
      "loss 18.67879259433462\n",
      "loss 21.3662160615255\n",
      "loss 6.503107174591062\n",
      "loss 21.366223429962126\n",
      "loss 16.214403317092096\n",
      "loss 16.21440362827165\n",
      "loss 16.21440393945177\n",
      "loss 6.503102481038649\n",
      "loss 18.6787959690416\n",
      "loss 21.36624553312332\n",
      "loss 6.503099665063372\n",
      "loss 21.366252900127574\n",
      "loss 16.21440580654437\n",
      "loss 16.21440611772845\n",
      "loss 16.214406428913097\n",
      "loss 6.503094972031496\n",
      "loss 18.678799343661783\n",
      "loss 21.36627499899228\n",
      "loss 6.5030921563685045\n",
      "loss 21.366282364564583\n",
      "loss 16.214408296032833\n",
      "loss 16.214408607221433\n",
      "loss 16.21440891841059\n",
      "loss 6.503087463857048\n",
      "loss 18.678802718195133\n",
      "loss 21.366304459134042\n",
      "loss 6.503084648506274\n",
      "loss 21.366311823274796\n",
      "loss 16.214410785557423\n",
      "loss 16.214411096750535\n",
      "loss 16.21441140794421\n",
      "loss 6.503079956515121\n",
      "loss 18.678806092641622\n",
      "loss 21.366333913550232\n",
      "loss 6.503077141476496\n",
      "loss 21.366341276259853\n",
      "loss 16.214413275118087\n",
      "loss 16.2144135863157\n",
      "loss 16.21441389751388\n",
      "loss 6.503072450005529\n",
      "loss 18.678809467001223\n",
      "loss 21.366363362242495\n",
      "loss 6.503069635278978\n",
      "loss 21.366370723521392\n",
      "loss 16.214415764714758\n",
      "loss 16.214416075916876\n",
      "loss 16.21441638711955\n",
      "loss 6.503064944328083\n",
      "loss 18.678812841273903\n",
      "loss 21.366392805212477\n",
      "loss 6.503062129913539\n",
      "loss 21.366400165061055\n",
      "loss 16.214418254347393\n",
      "loss 16.214418565554\n",
      "loss 16.214418876761165\n",
      "loss 6.503057439482596\n",
      "loss 18.678816215459634\n",
      "loss 21.366422242461812\n",
      "loss 6.50305462537999\n",
      "loss 21.366429600880487\n",
      "loss 16.214420744015925\n",
      "loss 16.214421055227014\n",
      "loss 16.214421366438664\n",
      "loss 6.503049935468884\n",
      "loss 18.678819589558383\n",
      "loss 21.366451673992138\n",
      "loss 6.503047121678145\n",
      "loss 21.366459030981314\n",
      "loss 16.214423233720296\n",
      "loss 16.214423544935865\n",
      "loss 16.21442385615199\n",
      "loss 6.503042432286761\n",
      "loss 18.678822963570124\n",
      "loss 21.3664810998051\n",
      "loss 6.50303961880782\n",
      "loss 21.366488455365193\n",
      "loss 16.214425723460458\n",
      "loss 16.214426034680493\n",
      "loss 16.21442634590108\n",
      "loss 6.503034929936039\n",
      "loss 18.678826337494822\n",
      "loss 21.366510519902327\n",
      "loss 6.503032116768827\n",
      "loss 21.36651787403374\n",
      "loss 16.21442821323634\n",
      "loss 16.214428524460835\n",
      "loss 16.214428835685887\n",
      "loss 6.503027428416534\n",
      "loss 18.67882971133245\n",
      "loss 21.36653993428546\n",
      "loss 6.503024615560978\n",
      "loss 21.36654728698861\n",
      "loss 16.214430703047896\n",
      "loss 16.214431014276844\n",
      "loss 16.214431325506347\n",
      "loss 6.503019927728056\n",
      "loss 18.67883308508298\n",
      "loss 21.36656934295614\n",
      "loss 6.5030171151840905\n",
      "loss 21.36657669423143\n",
      "loss 16.214433192895058\n",
      "loss 16.214433504128458\n",
      "loss 16.21443381536241\n",
      "loss 6.503012427870423\n",
      "loss 18.67883645874638\n",
      "loss 21.366598745915997\n",
      "loss 6.503009615637975\n",
      "loss 21.366606095763835\n",
      "loss 16.214435682777776\n",
      "loss 16.214435994015613\n",
      "loss 16.214436305254004\n",
      "loss 6.503004928843449\n",
      "loss 18.678839832322623\n",
      "loss 21.366628143166665\n",
      "loss 6.503002116922449\n",
      "loss 21.366635491587466\n",
      "loss 16.214438172695992\n",
      "loss 16.21443848393826\n",
      "loss 16.214438795181085\n",
      "loss 6.502997430646944\n",
      "loss 18.67884320581167\n",
      "loss 21.366657534709784\n",
      "loss 6.502994619037322\n",
      "loss 21.366664881703954\n",
      "loss 16.214440662649643\n",
      "loss 16.21444097389634\n",
      "loss 16.21444128514359\n",
      "loss 6.502989933280725\n",
      "loss 18.678846579213502\n",
      "loss 21.36668692054699\n",
      "loss 6.502987121982414\n",
      "loss 21.36669426611493\n",
      "loss 16.214443152638683\n",
      "loss 16.214443463889793\n",
      "loss 16.21444377514146\n",
      "loss 6.502982436744606\n",
      "loss 18.678849952528083\n",
      "loss 21.366716300679908\n",
      "loss 6.502979625757535\n",
      "loss 21.366723644822034\n",
      "loss 16.21444564266304\n",
      "loss 16.214445953918567\n",
      "loss 16.214446265174644\n",
      "loss 6.502974941038405\n",
      "loss 18.678853325755384\n",
      "loss 21.366745675110174\n",
      "loss 6.502972130362501\n",
      "loss 21.366753017826895\n",
      "loss 16.214448132722666\n",
      "loss 16.214448443982597\n",
      "loss 16.21444875524308\n",
      "loss 6.502967446161927\n",
      "loss 18.678856698895377\n",
      "loss 21.366775043839425\n",
      "loss 6.5029646357971265\n",
      "loss 21.366782385131142\n",
      "loss 16.214450622817505\n",
      "loss 16.21445093408183\n",
      "loss 16.21445124534671\n",
      "loss 6.5029599521149954\n",
      "loss 18.67886007194803\n",
      "loss 21.366804406869285\n",
      "loss 6.5029571420612235\n",
      "loss 21.366811746736413\n",
      "loss 16.214453112947496\n",
      "loss 16.214453424216213\n",
      "loss 16.214453735485478\n",
      "loss 6.502952458897419\n",
      "loss 18.678863444913315\n",
      "loss 21.36683376420139\n",
      "loss 6.502949649154608\n",
      "loss 21.366841102644333\n",
      "loss 16.214455603112583\n",
      "loss 16.21445591438568\n",
      "loss 16.21445622565933\n",
      "loss 6.502944966509016\n",
      "loss 18.678866817791203\n",
      "loss 21.366863115837376\n",
      "loss 6.502942157077097\n",
      "loss 21.36687045285654\n",
      "loss 16.214458093312707\n",
      "loss 16.214458404590182\n",
      "loss 16.214458715868204\n",
      "loss 6.502937474949598\n",
      "loss 18.67887019058166\n",
      "loss 21.366892461778857\n",
      "loss 6.502934665828502\n",
      "loss 21.36689979737465\n",
      "loss 16.214460583547808\n",
      "loss 16.214460894829656\n",
      "loss 16.214461206112045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 6.502929984218982\n",
      "loss 18.67887356328466\n",
      "loss 21.366921802027473\n",
      "loss 6.502927175408638\n",
      "loss 21.36692913620031\n",
      "loss 16.214463073817843\n",
      "loss 16.214463385104047\n",
      "loss 16.214463696390798\n",
      "loss 6.502922494316981\n",
      "loss 18.678876935900178\n",
      "loss 21.366951136584852\n",
      "loss 6.50291968581732\n",
      "loss 21.36695846933513\n",
      "loss 16.214465564122737\n",
      "loss 16.214465875413296\n",
      "loss 16.214466186704403\n",
      "loss 6.502915005243412\n",
      "loss 18.678880308428173\n",
      "loss 21.36698046545262\n",
      "loss 6.502912197054364\n",
      "loss 21.36698779678075\n",
      "loss 16.214468054462447\n",
      "loss 16.214468365757355\n",
      "loss 16.214468677052803\n",
      "loss 6.502907516998087\n",
      "loss 18.678883680868623\n",
      "loss 21.36700978863241\n",
      "loss 6.5029047091195835\n",
      "loss 21.367017118538797\n",
      "loss 16.214470544836907\n",
      "loss 16.214470856136153\n",
      "loss 16.214471167435946\n",
      "loss 6.502900029580822\n",
      "loss 18.678887053221494\n",
      "loss 21.367039106125837\n",
      "loss 6.502897222012793\n",
      "loss 21.36704643461089\n",
      "loss 16.214473035246066\n",
      "loss 16.214473346549642\n",
      "loss 16.214473657853766\n",
      "loss 6.502892542991432\n",
      "loss 18.678890425486763\n",
      "loss 21.367068417934536\n",
      "loss 6.50288973573381\n",
      "loss 21.367075744998665\n",
      "loss 16.21447552568986\n",
      "loss 16.214475836997767\n",
      "loss 16.214476148306215\n",
      "loss 6.502885057229732\n",
      "loss 18.678893797664397\n",
      "loss 21.36709772406013\n",
      "loss 6.502882250282445\n",
      "loss 21.367105049703735\n",
      "loss 16.214478016168236\n",
      "loss 16.214478327480464\n",
      "loss 16.214478638793228\n",
      "loss 6.502877572295537\n",
      "loss 18.678897169754364\n",
      "loss 21.367127024504246\n",
      "loss 6.502874765658516\n",
      "loss 21.367134348727735\n",
      "loss 16.214480506681138\n",
      "loss 16.21448081799768\n",
      "loss 16.214481129314755\n",
      "loss 6.502870088188663\n",
      "loss 18.678900541756636\n",
      "loss 21.367156319268506\n",
      "loss 6.50286728186184\n",
      "loss 21.367163642072285\n",
      "loss 16.21448299722851\n",
      "loss 16.214483308549354\n",
      "loss 16.214483619870734\n",
      "loss 6.502862604908923\n",
      "loss 18.678903913671185\n",
      "loss 21.367185608354532\n",
      "loss 6.50285979889223\n",
      "loss 21.367192929739012\n",
      "loss 16.2144854878103\n",
      "loss 16.214485799135435\n",
      "loss 16.214486110461117\n",
      "loss 6.502855122456134\n",
      "loss 18.67890728549798\n",
      "loss 21.367214891763954\n",
      "loss 6.5028523167494985\n",
      "loss 21.36722221172953\n",
      "loss 16.214487978426437\n",
      "loss 16.214488289755867\n",
      "loss 16.214488601085833\n",
      "loss 6.502847640830112\n",
      "loss 18.678910657236994\n",
      "loss 21.367244169498388\n",
      "loss 6.502844835433465\n",
      "loss 21.36725148804547\n",
      "loss 16.214490469076875\n",
      "loss 16.21449078041059\n",
      "loss 16.214491091744836\n",
      "loss 6.50284016003067\n",
      "loss 18.678914028888194\n",
      "loss 21.367273441559455\n",
      "loss 6.502837354943946\n",
      "loss 21.367280758688448\n",
      "loss 16.214492959761554\n",
      "loss 16.214493271099542\n",
      "loss 16.214493582438067\n",
      "loss 6.502832680057624\n",
      "loss 18.678917400451553\n",
      "loss 21.36730270794878\n",
      "loss 6.502829875280749\n",
      "loss 21.367310023660096\n",
      "loss 16.214495450480417\n",
      "loss 16.214495761822675\n",
      "loss 16.214496073165467\n",
      "loss 6.502825200910791\n",
      "loss 18.678920771927043\n",
      "loss 21.367331968667987\n",
      "loss 6.502822396443699\n",
      "loss 21.36733928296202\n",
      "loss 16.21449794123341\n",
      "loss 16.21449825257993\n",
      "loss 16.214498563926984\n",
      "loss 6.502817722589986\n",
      "loss 18.67892414331463\n",
      "loss 21.36736122371869\n",
      "loss 6.502814918432605\n",
      "loss 21.367368536595848\n",
      "loss 16.214500432020476\n",
      "loss 16.21450074337125\n",
      "loss 16.214501054722557\n",
      "loss 6.502810245095024\n",
      "loss 18.67892751461429\n",
      "loss 21.367390473102503\n",
      "loss 6.502807441247288\n",
      "loss 21.367397784563195\n",
      "loss 16.21450292284155\n",
      "loss 16.214503234196577\n",
      "loss 16.21450354555213\n",
      "loss 6.50280276842572\n",
      "loss 18.678930885825988\n",
      "loss 21.367419716821058\n",
      "loss 6.502799964887558\n",
      "loss 21.367427026865688\n",
      "loss 16.21450541369659\n",
      "loss 16.214505725055854\n",
      "loss 16.21450603641565\n",
      "loss 6.502795292581892\n",
      "loss 18.6789342569497\n",
      "loss 21.367448954875968\n",
      "loss 6.502792489353235\n",
      "loss 21.367456263504938\n",
      "loss 16.214507904585528\n",
      "loss 16.214508215949024\n",
      "loss 16.214508527313054\n",
      "loss 6.502787817563352\n",
      "loss 18.67893762798539\n",
      "loss 21.367478187268844\n",
      "loss 6.502785014644134\n",
      "loss 21.367485494482565\n",
      "loss 16.214510395508313\n",
      "loss 16.214510706876034\n",
      "loss 16.21451101824429\n",
      "loss 6.50278034336992\n",
      "loss 18.67894099893304\n",
      "loss 21.367507414001313\n",
      "loss 6.502777540760068\n",
      "loss 21.36751471980018\n",
      "loss 16.214512886464885\n",
      "loss 16.21451319783683\n",
      "loss 16.214513509209304\n",
      "loss 6.502772870001409\n",
      "loss 18.67894436979261\n",
      "loss 21.367536635074984\n",
      "loss 6.502770067700855\n",
      "loss 21.36754393945941\n",
      "loss 16.214515377455193\n",
      "loss 16.214515688831348\n",
      "loss 16.214516000208032\n",
      "loss 6.502765397457636\n",
      "loss 18.678947740564077\n",
      "loss 21.367565850491477\n",
      "loss 6.5027625954663115\n",
      "loss 21.36757315346186\n",
      "loss 16.214517868479174\n",
      "loss 16.214518179859535\n",
      "loss 16.214518491240423\n",
      "loss 6.5027579257384165\n",
      "loss 18.678951111247407\n",
      "loss 21.367595060252413\n",
      "loss 6.502755124056253\n",
      "loss 21.36760236180915\n",
      "loss 16.214520359536774\n",
      "loss 16.214520670921335\n",
      "loss 16.214520982306418\n",
      "loss 6.502750454843568\n",
      "loss 18.678954481842574\n",
      "loss 21.367624264359392\n",
      "loss 6.502747653470497\n",
      "loss 21.367631564502894\n",
      "loss 16.214522850627937\n",
      "loss 16.214523162016693\n",
      "loss 16.21452347340597\n",
      "loss 6.502742984772905\n",
      "loss 18.678957852349548\n",
      "loss 21.367653462814037\n",
      "loss 6.502740183708855\n",
      "loss 21.367660761544712\n",
      "loss 16.214525341752612\n",
      "loss 16.214525653145547\n",
      "loss 16.214525964539007\n",
      "loss 6.502735515526245\n",
      "loss 18.6789612227683\n",
      "loss 21.367682655617962\n",
      "loss 6.502732714771148\n",
      "loss 21.367689952936207\n",
      "loss 16.214527832910733\n",
      "loss 16.214528144307845\n",
      "loss 16.214528455705484\n",
      "loss 6.502728047103402\n",
      "loss 18.678964593098804\n",
      "loss 21.367711842772774\n",
      "loss 6.5027252466571905\n",
      "loss 21.367719138678993\n",
      "loss 16.214530324102245\n",
      "loss 16.214530635503532\n",
      "loss 16.214530946905334\n",
      "loss 6.502720579504196\n",
      "loss 18.678967963341027\n",
      "loss 21.367741024280093\n",
      "loss 6.5027177793668\n",
      "loss 21.36774831877469\n",
      "loss 16.214532815327104\n",
      "loss 16.214533126732547\n",
      "loss 16.214533438138517\n",
      "loss 6.50271311272844\n",
      "loss 18.67897133349494\n",
      "loss 21.367770200141525\n",
      "loss 6.50271031289979\n",
      "loss 21.3677774932249\n",
      "loss 16.214535306585237\n",
      "loss 16.21453561799484\n",
      "loss 16.214535929404963\n",
      "loss 6.5027056467759525\n",
      "loss 18.678974703560513\n",
      "loss 21.367799370358682\n",
      "loss 6.50270284725598\n",
      "loss 21.36780666203124\n",
      "loss 16.2145377978766\n",
      "loss 16.214538109290352\n",
      "loss 16.214538420704624\n",
      "loss 6.502698181646548\n",
      "loss 18.678978073537724\n",
      "loss 21.367828534933174\n",
      "loss 6.502695382435185\n",
      "loss 21.36783582519532\n",
      "loss 16.214540289201132\n",
      "loss 16.214540600619028\n",
      "loss 16.21454091203744\n",
      "loss 6.502690717340045\n",
      "loss 18.678981443426537\n",
      "loss 21.36785769386662\n",
      "loss 6.502687918437221\n",
      "loss 21.36786498271875\n",
      "loss 16.214542780558773\n",
      "loss 16.214543091980804\n",
      "loss 16.21454340340335\n",
      "loss 6.502683253856259\n",
      "loss 18.678984813226922\n",
      "loss 21.36788684716061\n",
      "loss 6.502680455261908\n",
      "loss 21.367894134603134\n",
      "loss 16.214545271949476\n",
      "loss 16.214545583375635\n",
      "loss 16.21454589480231\n",
      "loss 6.502675791195006\n",
      "loss 18.678988182938856\n",
      "loss 21.367915994816766\n",
      "loss 6.502672992909058\n",
      "loss 21.367923280850082\n",
      "loss 16.21454776337318\n",
      "loss 16.21454807480346\n",
      "loss 16.214548386234256\n",
      "loss 6.502668329356105\n",
      "loss 18.678991552562305\n",
      "loss 21.367945136836695\n",
      "loss 6.502665531378491\n",
      "loss 21.367952421461204\n",
      "loss 16.21455025482983\n",
      "loss 16.214550566264222\n",
      "loss 16.21455087769913\n",
      "loss 6.502660868339369\n",
      "loss 18.678994922097242\n",
      "loss 21.367974273222003\n",
      "loss 6.502658070670023\n",
      "loss 21.36798155643811\n",
      "loss 16.214552746319363\n",
      "loss 16.214553057757865\n",
      "loss 16.21455336919688\n",
      "loss 6.502653408144619\n",
      "loss 18.678998291543643\n",
      "loss 21.368003403974296\n",
      "loss 6.502650610783469\n",
      "loss 21.368010685782398\n",
      "loss 16.214555237841736\n",
      "loss 16.214555549284338\n",
      "loss 16.21455586072745\n",
      "loss 6.50264594877167\n",
      "loss 18.67900166090147\n",
      "loss 21.368032529095185\n",
      "loss 6.502643151718648\n",
      "loss 21.36803980949568\n",
      "loss 16.21455772939688\n",
      "loss 16.214558040843578\n",
      "loss 16.214558352290787\n",
      "loss 6.50263849022034\n",
      "loss 18.6790050301707\n",
      "loss 21.368061648586263\n",
      "loss 6.502635693475376\n",
      "loss 21.368068927579564\n",
      "loss 16.21456022098475\n",
      "loss 16.214560532435534\n",
      "loss 16.214560843886826\n",
      "loss 6.502631032490445\n",
      "loss 18.679008399351304\n",
      "loss 21.36809076244915\n",
      "loss 6.5026282360534715\n",
      "loss 21.36809804003565\n",
      "loss 16.214562712605286\n",
      "loss 16.214563024060148\n",
      "loss 16.214563335515518\n",
      "loss 6.502623575581799\n",
      "loss 18.67901176844325\n",
      "loss 21.36811987068544\n",
      "loss 6.5026207794527515\n",
      "loss 21.368127146865547\n",
      "loss 16.21456520425843\n",
      "loss 16.214565515717364\n",
      "loss 16.214565827176806\n",
      "loss 6.502616119494226\n",
      "loss 18.679015137446513\n",
      "loss 21.36814897329674\n",
      "loss 6.502613323673031\n",
      "loss 21.368156248070846\n",
      "loss 16.21456769594413\n",
      "loss 16.21456800740713\n",
      "loss 16.214568318870636\n",
      "loss 6.502608664227539\n",
      "loss 18.679018506361064\n",
      "loss 21.368178070284657\n",
      "loss 6.502605868714127\n",
      "loss 21.368185343653167\n",
      "loss 16.214570187662325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 16.214570499129387\n",
      "loss 16.214570810596946\n",
      "loss 6.502601209781554\n",
      "loss 18.67902187518687\n",
      "loss 21.368207161650787\n",
      "loss 6.502598414575861\n",
      "loss 21.368214433614103\n",
      "loss 16.214572679412967\n",
      "loss 16.214572990884076\n",
      "loss 16.214573302355692\n",
      "loss 6.502593756156092\n",
      "loss 18.679025243923906\n",
      "loss 21.368236247396737\n",
      "loss 6.502590961258047\n",
      "loss 21.368243517955257\n",
      "loss 16.214575171195992\n",
      "loss 16.214575482671144\n",
      "loss 16.214575794146803\n",
      "loss 6.5025863033509665\n",
      "loss 18.67902861257214\n",
      "loss 21.368265327524107\n",
      "loss 6.502583508760503\n",
      "loss 21.36827259667823\n",
      "loss 16.214577663011347\n",
      "loss 16.21457797449054\n",
      "loss 16.214578285970234\n",
      "loss 6.502578851365998\n",
      "loss 18.67903198113155\n",
      "loss 21.368294402034497\n",
      "loss 6.502576057083045\n",
      "loss 21.36830166978463\n",
      "loss 16.21458015485898\n",
      "loss 16.2145804663422\n",
      "loss 16.214580777825923\n",
      "loss 6.5025714002010035\n",
      "loss 18.6790353496021\n",
      "loss 21.368323470929504\n",
      "loss 6.502568606225493\n",
      "loss 21.368330737276043\n",
      "loss 16.21458264673883\n",
      "loss 16.214582958226075\n",
      "loss 16.21458326971382\n",
      "loss 6.5025639498558\n",
      "loss 18.679038717983765\n",
      "loss 21.368352534210732\n",
      "loss 6.502561156187664\n",
      "loss 21.368359799154078\n",
      "loss 16.214585138650843\n",
      "loss 16.214585450142103\n",
      "loss 16.214585761633867\n",
      "loss 6.502556500330205\n",
      "loss 18.67904208627652\n",
      "loss 21.368381591879785\n",
      "loss 6.502553706969374\n",
      "loss 21.368388855420328\n",
      "loss 16.214587630594966\n",
      "loss 16.214587942090237\n",
      "loss 16.214588253586008\n",
      "loss 6.502549051624035\n",
      "loss 18.679045454480327\n",
      "loss 21.36841064393825\n",
      "loss 6.502546258570442\n",
      "loss 21.368417906076402\n",
      "loss 16.214590122571142\n",
      "loss 16.214590434070416\n",
      "loss 16.214590745570185\n",
      "loss 6.50254160373711\n",
      "loss 18.679048822595167\n",
      "loss 21.36843969038773\n",
      "loss 6.5025388109906865\n",
      "loss 21.368446951123886\n",
      "loss 16.214592614579317\n",
      "loss 16.214592926082585\n",
      "loss 16.214593237586353\n",
      "loss 6.502534156669245\n",
      "loss 18.679052190621007\n",
      "loss 21.368468731229825\n",
      "loss 6.502531364229924\n",
      "loss 21.36847599056438\n",
      "loss 16.21459510661943\n",
      "loss 16.214595418126688\n",
      "loss 16.214595729634443\n",
      "loss 6.502526710420261\n",
      "loss 18.679055558557817\n",
      "loss 21.36849776646613\n",
      "loss 6.502523918287972\n",
      "loss 21.368505024399486\n",
      "loss 16.21459759869143\n",
      "loss 16.21459791020267\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.003\n",
    "num_of_iterations = 100\n",
    "batch_size = len(X_sample)\n",
    "training_loss = run(X_sample, Y_sample, alpha, num_of_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### plot the training losses \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(training_loss, range(batch_size*num_of_iterations),label='training_loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ALT OPERATIONS CODE\n",
    "\n",
    "#for index in np.ndindex(X.shape):\n",
    "                    #    if X[index] == np.amax(X[stride_y:stride_y+row_id, stride_x:stride_x+col_id]):\n",
    "                    #        if index not in index_list:\n",
    "                    #            index_list.append(index)\n",
    "                    #3#3            temp_list.append([X[index],index])\n",
    "                    #            break\n",
    "                    #3        else:\n",
    "                    #3            print X[index], index\n",
    "                    #        continue    \n",
    "                    \n",
    "                    \n",
    "                                        \n",
    "                    #for row in range(stride_y,stride_y+row_id):\n",
    "                    #    l=1\n",
    "                    #    print 'row',row\n",
    "                    #    for col in range(stride_x,stride_x+col_id):\n",
    "                    #        print 'col',col\n",
    "                    #        if X[row,col] == np.amax(X[row:row+row_id, col:col+col_id]):\n",
    "                    #            index_list.append((row,col))\n",
    "                    #            l=0\n",
    "                    #            break\n",
    "                    #    if l == 0:\n",
    "                    #        break\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                                        #for index in np.ndindex(X.shape):\n",
    "                    #    if X[index] == np.amax(X[stride_y:stride_y+row_id, stride_x:stride_x+col_id]):\n",
    "                    #        if index[1] < stride_x + col_id:\n",
    "                    #            if index[0] < stride_y + row_id:\n",
    "                    #                index_list.append([X[index],index])\n",
    "                    #                continue\n",
    "\n",
    "                    \n",
    "                                        #temp1 = np.equal(X,\\\n",
    "                                    # np.amax(X[stride_y:stride_y+row_id, stride_x:stride_x+col_id]))\n",
    "                    #print '%d \\n' %(stride_y), temp1\n",
    "                    \n",
    "                    #temp = np.argmax(temp1==True)\n",
    "                    \n",
    "                    \n",
    "#for num in range(len(X_sample)):\n",
    "#for i, j in itertools.izip(X_sample,Y_sample):\n",
    "#    print i\n",
    "#    break\n",
    "\n",
    "#import numpy as np#\n",
    "\n",
    "#stride = 1\n",
    "\n",
    "#X_sample = np.array([[1,1,1,1,1],[2,2,2,2,2],[3,3,3,3,3],[4,4,4,4,4],[5,5,5,5,5]])\n",
    "#grad_activation_map = np.array([[2,2,2,2,2],[4,4,4,4,4],[6,6,6,6,6],[8,8,8,8,8],[10,10,10,10,10]])\n",
    "#kernel = np.zeros((3,3))\n",
    "#d_kernel = kernel.copy()\n",
    "#row_id = kernel.shape[0]\n",
    "#col_id = kernel.shape[1]\n",
    "\n",
    "#for stride_y in range(0, grad_activation_map.shape[0], stride):\n",
    "#        for stride_x in range(0, grad_activation_map.shape[1], stride):\n",
    "#            if stride_y + row_id <= grad_activation_map.shape[0]:\n",
    "#                if stride_x + col_id <= grad_activation_map.shape[1]:\n",
    "#                    temp = X_sample[stride_y:stride_y+row_id, stride_x:stride_x+col_id]*\\\n",
    "#                    grad_activation_map[stride_y:stride_y+row_id, stride_x:stride_x+col_id]\n",
    "#                    d_kernel = d_kernel + temp\n",
    "#                    #print '\\ntemp',temp, '\\n'\n",
    "#                    del temp\n",
    "                    #print 'd_kernel \\n', d_kernel\n",
    "\n",
    "#d_kernel = np.asarray(d_kernel).reshape(X_sample.shape[0],X_sample.shape[1]).sum(axis=0).sum(axis=1)\n",
    "#d_kernel = np.asarray(d_kernel).reshape(X_sample.shape[0],X_sample.shape[1])\n",
    "#print 'd_kernel shape',d_kernel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
